{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Programs\\Anaconda\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras import backend\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#from keras.models import Sequential\n",
    "#from keras.layers import Dense, Dropout, Flatten\n",
    "#from keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.python.keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "datagen = ImageDataGenerator(rescale=1. / 255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Каталог с данными для обучения\n",
    "train_dir = 'numbers/train'\n",
    "# Каталог с данными для проверки\n",
    "val_dir = 'numbers/val'\n",
    "# Каталог с данными для тестирования\n",
    "test_dir = 'numbers/test'\n",
    "# Размеры изображения\n",
    "img_width, img_height = 28, 28\n",
    "# Размерность тензора на основе изображения для входных данных в нейронную сеть\n",
    "input_shape = (img_width, img_height, 3)\n",
    "# Количество эпох\n",
    "epochs = 30\n",
    "# Размер мини-выборки\n",
    "batch_size = 16\n",
    "# Количество изображений для обучения\n",
    "nb_train_samples = 25506\n",
    "# Количество изображений для проверки\n",
    "nb_validation_samples = 4500\n",
    "# Количество изображений для тестирования\n",
    "nb_test_samples = 4500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 25506 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "train_generator = datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(28, 28),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4500 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "val_generator = datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4500 images belonging to 9 classes.\n"
     ]
    }
   ],
   "source": [
    "test_generator = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(img_width, img_height),\n",
    "    batch_size=batch_size,\n",
    "    class_mode='categorical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создаем последовательную модель\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Conv2D(75, kernel_size=(5, 5),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Conv2D(100, (5, 5), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(500, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(9, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методу fit_generator передаем два генератора: train_generator с данными для обучения и val_generator с данными для проверки. Обучение выполняется в течение 30 эпох.\n",
    "\n",
    "Генераторы в Keras устроены так, что могут выдавать изображения бесконечно. После того, как изображения в каталоге закончатся, происходит переход в начало каталога, и генератор начинает работать снова. Поэтому нужно указать, сколько будет обращений к генератору на каждой эпохе обучения. Для этого используется параметры steps_per_epoch (данные для обучения) и validation_steps (данные для проверки). За одно обращение генератор выдает не одно изображение, а несколько, в соответствии с размером его мини-выборки (batch_size). Чтобы рассчитать количество обращений к генератору, при котором мы сможем получить все изображения из набора данных по одному разу, мы делим количество изображений в наборе на размер мини-выборки."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "1594/1594 [==============================] - 54s 34ms/step - loss: 0.1338 - acc: 0.9650 - val_loss: 0.0381 - val_acc: 0.9944\n",
      "Epoch 2/3\n",
      "1594/1594 [==============================] - 50s 32ms/step - loss: 0.0437 - acc: 0.9936 - val_loss: 0.0363 - val_acc: 0.9964\n",
      "Epoch 3/3\n",
      "1594/1594 [==============================] - 51s 32ms/step - loss: 0.0434 - acc: 0.9947 - val_loss: 0.0439 - val_acc: 0.9938\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x217e344f7f0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "#early_stopping_callback = EarlyStopping(monitor='val_acc', patience=4)\n",
    "\n",
    "model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch=nb_train_samples // batch_size,\n",
    "    epochs=3,\n",
    "    validation_data=val_generator,\n",
    "    validation_steps=nb_validation_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аккуратность на тестовых данных: 99.35%\n"
     ]
    }
   ],
   "source": [
    "scores = model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n",
    "print(\"Аккуратность на тестовых данных: %.2f%%\" % (scores[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 24, 24, 75)        5700      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 12, 12, 75)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 12, 12, 75)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 8, 8, 100)         187600    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 4, 4, 100)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1600)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 500)               800500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 9)                 4509      \n",
      "=================================================================\n",
      "Total params: 998,309\n",
      "Trainable params: 998,309\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x = image.load_img(\"1.jpg\", target_size=(28, 28))\n",
    "plt.imshow(x)\n",
    "plt.show()\n",
    "x = image.img_to_array(x)\n",
    "x = x.astype('float32')\n",
    "x /= 255\n",
    "#x = x[np.newaxis, :, :, :]\n",
    "x = np.expand_dims(x, axis=0)\n",
    "prediction = model.predict(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "print(np.argmax(prediction) + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Генерируем описание модели в формате json\n",
    "model_json = model.to_json()\n",
    "# Записываем модель в файл\n",
    "json_file = open(\"my_model.json\", \"w\")\n",
    "json_file.write(model_json)\n",
    "json_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights(\"my_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для загрузки сети, как и для сохранения, нам необходимо выполнить две операции:\n",
    "\n",
    "    Загрузить данные об архитектуре сети.\n",
    "    Загрузить данные о весах.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import model_from_json\n",
    "\n",
    "# Загружаем данные об архитектуре сети из файла json\n",
    "json_file = open(\"my_model.json\", \"r\")\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "# Создаем модель на основе загруженных данных\n",
    "loaded_model = model_from_json(loaded_model_json)\n",
    "# Загружаем веса в модель\n",
    "loaded_model.load_weights(\"my_model.h5\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед использованием модели, ее обязательно нужно скомпилировать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность модели на тестовых данных: 99.76%\n"
     ]
    }
   ],
   "source": [
    "# Компилируем модель\n",
    "loaded_model.compile(loss=\"categorical_crossentropy\", optimizer=\"SGD\", metrics=[\"accuracy\"])\n",
    "# Проверяем модель на тестовых данных\n",
    "scores = loaded_model.evaluate_generator(test_generator, nb_test_samples // batch_size)\n",
    "print(\"Точность модели на тестовых данных: %.2f%%\" % (scores[1]*100))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
